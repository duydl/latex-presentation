\documentclass[11pt,xcolor={dvipsnames},hyperref={pdftex,pdfpagemode=UseNone,hidelinks,pdfdisplaydoctitle=true},usepdftitle=false]{beamer}
\usepackage{presentation}
% Enter title of presentation PDF:
\hypersetup{pdftitle={Minimalist LaTeX Template for Academic Presentations}}
% Enter link to PDF file with figures:
\newcommand{\pdf}{figures.pdf}

\begin{document}
% Enter presentation title:
\title{\textbf{BERT}: Pre-training of Deep Bidirectional Transformers for \\ Language Understanding}
\information%
[https://aclanthology.org/N19-1423.pdf]%
{ Google AI Language}%
% {Location -- Date}
\frame{\titlepage}

% Intro
\begin{frame}
\frametitle{Introduction}
\textbf{Context and Motivation:}
\begin{itemize}
    \item Pre-training language models has significantly advanced the performance of many NLP tasks.
\end{itemize}
\textbf{Existing Strategies:}
\begin{itemize}
    \item Feature-based approach (e.g., ELMo): Integrates pre-trained representations as additional features into task-specific architectures.
    \item Fine-tuning approach (e.g., OpenAI GPT): Adapts to downstream tasks with minimal task-specific parameters by fine-tuning all layers.
\end{itemize}
\textbf{Limitations of Current Techniques:}
\begin{itemize}
    \item Constrained by unidirectionality:  Restricts learning of contextual relationships essential for comprehensive language understanding, especially in token/sentence-level tasks such
    as question answering.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{BERT: Bidirectional Encoder Representations from Transformers}
\textbf{Key Innovations of BERT:}
\begin{itemize}
    \item Using a "masked language model" (MLM) pre-training objective for deep bidirectional Transformer.
    \item Also incorporates a "next sentence prediction" task that jointly pre-trains text-pair representations.
\end{itemize}
\textbf{Contributions and Impact:}
\begin{itemize}
    \item Demonstrates bidirectional pre-training for language representations.
    \item Simplifies the architecture requirements for specific NLP tasks.
    \item Achieves state-of-the-art performance across eleven tasks.
\end{itemize}
\end{frame}
    

% Related
\begin{frame}
\frametitle{Related Work}

\textbf{Unsupervised Feature-based Approaches:}
\begin{itemize}
    \item Initial focus on word embeddings
    % (Brown et al., 1992; Mikolov et al., 2013).
    % \item Evolution with various training objectives, including:
    % \begin{itemize}
    %     \item Ranking candidate next sentences.
    %     \item Generating next sentence words from sentence representations.
    %     \item Denoising auto-encoder objectives.
    % \end{itemize}
    \item ELMo introduced context-sensitive features from bidirectional language models, significantly advancing NLP benchmarks.
\end{itemize}

\textbf{Unsupervised Fine-tuning Approaches:}
\begin{itemize}
    \item Early models only pre-trained word embeddings 
    % (Collobert and Weston, 2008).
    \item Development of contextual token representations pre-trained and fine-tuned from unlabeled text for downstream tasks:
    % \begin{itemize}
    %     \item Combines advantages of fewer learn-from-scratch parameters and robust context awareness.
    %     \item OpenAI GPT exemplifies success on the GLUE benchmark via left-to-right modeling.
    % \end{itemize}
\end{itemize}

\textbf{Transfer Learning from Supervised Data:}
% \begin{itemize}
%     \item Demonstrated effectiveness in NLP tasks with large datasets (e.g., natural language inference, machine translation).
%     \item Mirrored in computer vision, where fine-tuning from models pre-trained on large datasets like ImageNet has been crucial.
% \end{itemize}

\end{frame}

% BERT %
% Slide 1: Overview of BERT's Framework
\begin{frame}
    \frametitle{Overview of BERT}
    \begin{itemize}
        \item Handles language representations through two main phases: \textbf{pre-training} and \textbf{fine-tuning}.
        \item \textbf{Pre-training}
            \begin{itemize}
                \item Trained on a large corpus of unlabeled data.
                \item Employs two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).
                \item Captures deep bidirectional context of language.
            \end{itemize}
        \item \textbf{Fine-tuning}
            \begin{itemize}
                \item Uses labeled data specific to downstream tasks.
                \item Adapts BERT's architecture to achieve state-of-the-art results.
                \item Applicable to a wide range of NLP challenges.
            \end{itemize}
    \end{itemize}
\end{frame}

% Slide 2: BERT Model Architecture
\begin{frame}
\frametitle{BERT Model Architecture}
BERT based on the original Transformer model but modified to support bidirectional context processing. Key configurations include:
\begin{itemize}
    \item Number of layers (L)  e.g., BERT BASE has 12 layers.
    \item Hidden size (H): size of the hidden layers. BERT BASE uses 768.
    \item Self-attention heads (A): BERT BASE uses 12 attention heads.
    \item Total Parameters: BERT BASE has 110M parameters.
\end{itemize}
Unlike OpenAI GPT unidirectional architecture, BERT's bidirectional Transformer encoder allows each token to attend to all tokens in the input sequence.
\end{frame}

% Slide 3: Input/Output Representations
\begin{frame}
\frametitle{Input/Output Representations}
Designed to handle single sentences or a pair of sentences seamlessly by incorporating special tokens and embeddings:
\begin{itemize}
    \item Special tokens: {\tt [CLS]} for classification tasks and {\tt [SEP]} for separating sentence pairs.
    \item Segment embeddings: distinguish between sentences in tasks involving comparisons.
    \item Positional embeddings: maintain the positional context of words.
\end{itemize}
The sum of these embeddings provides a rich representation of the input tokens, which is critical for the model to understand the language context fully.
% \begin{figure}
% \centering
% \includegraphics[width=0.8\linewidth]{Input_Emebeddings.pdf}
% \caption{Illustration of BERT input representation construction.}
% \end{figure}
\end{frame}

% Slide 4: Pre-training Tasks
\begin{frame}
\frametitle{Pre-training Tasks of BERT}
BERT is pre-trained using two unsupervised tasks:
\begin{itemize}
    \item \textbf{Masked LM (MLM)}: Learns to predict randomly masked tokens in input based on the context provided by the non-masked tokens.
    \item \textbf{Next Sentence Prediction (NSP)}: Learns to predict if a sentence logically follows another.
\end{itemize}
\end{frame}

% Slide 5: Fine-tuning Procedure
\begin{frame}
\frametitle{Fine-tuning Procedure}
Can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks. The fine-tuning adjusts all the pre-trained parameters to make them task-specific:
\begin{itemize}
    \item Input during fine-tuning is adjusted according to the task (e.g., sentence pairs for NLI).
    \item The model outputs are tailored to the specific needs of the task, using the representations learned during pre-training.
\end{itemize}
\end{frame}
    
\begin{frame}
\frametitle{GLUE Benchmark}
The GLUE benchmark evaluates models' language understanding across diverse NLP tasks like question answering and sentiment analysis. BERT, fine-tuned on these tasks, uses pre-trained representations with a simple classification layer for predictions. Fine-tuning BERT involves:
\begin{itemize}
    \item Using the {\tt [CLS]} token's final hidden state for classification.
    \item Adding task-specific parameters, primarily the classification layer.
    \item Optimizing on each task's training data with task-specific hyperparameters.
\end{itemize}
BERT's performance on GLUE exceeds previous state-of-the-art models, showcasing its transfer learning capabilities.
\end{frame}
    

\begin{frame}
\frametitle{SQuAD v1.1}
The Stanford Question Answering Dataset (SQuAD v1.1) challenges models to answer questions based on content from Wikipedia articles, where the answer to each question is a segment of text, or "span", from the corresponding reading passage.
\begin{itemize}
    \item BERT reformulates question answering as a span prediction task.
    \item It predicts the start and the end of the answer span within the passage.
    \item The model is fine-tuned to maximize the log-probability of the correct answer span.
\end{itemize}
BERT's fine-tuning approach allows it to outperform previous models on SQuAD v1.1, achieving new state-of-the-art results.
\end{frame}

\begin{frame}
\frametitle{SQuAD v2.0}
SQuAD v2.0 extends v1.1 by adding questions that do not have an answer in the provided passage, making it essential for models to determine not only the answers but also when no answer is supported by the text.
\begin{itemize}
    \item For no-answer predictions, BERT compares the score of a null answer (based on the {\tt [CLS]} token) to the best non-null span score.
    \item A threshold value ($\tau$) is tuned on the development set to decide when to predict no answer.
\end{itemize}
This adjustment makes SQuAD v2.0 a more challenging and realistic task, which BERT handles effectively, significantly improving over prior best models.
\end{frame}


\begin{frame}
\frametitle{SWAG}
The Situations With Adversarial Generations (SWAG) dataset aims to evaluate a model's ability to predict the most plausible continuation of a sentence among four given options.
\begin{itemize}
    \item BERT is fine-tuned to select the most plausible sentence continuation, employing its pre-trained contextual understanding enhanced with the SWAG dataset.
    \item Performance on SWAG highlights BERT's capacity for commonsense reasoning and contextual inference.
\end{itemize}
BERT's results on SWAG dramatically surpass previous approaches, underlining its robustness in handling complex language understanding tasks.
\end{frame}

% Concise Slide: Ablation Studies on BERT
\begin{frame}
    \frametitle{Ablation Studies on BERT}
    \begin{itemize}
        \item \textbf{Pre-training Tasks:}
        \begin{itemize}
            \item Removing NSP decreases performance on tasks needing sentence relationship understanding (e.g., QNLI, MNLI).
            \item Bidirectional models (MLM) outperform unidirectional models (LTR) across tasks, especially SQuAD and MRPC.
        \end{itemize}
        \item \textbf{Model Size:}
        \begin{itemize}
            \item Larger models improve performance on GLUE tasks.
            \item Scaling enhances feature and dependency capture, even for small-scale tasks.
        \end{itemize}
        \item \textbf{Feature-based vs. Fine-tuning:}
        \begin{itemize}
            \item Feature-based methods using BERT's layers are competitive, especially in NER task.
            \item Fine-tuning generally outperforms.
        \end{itemize}
    \end{itemize}
\end{frame}
    

\end{document}