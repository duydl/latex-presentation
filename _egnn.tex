\documentclass[11pt,xcolor={dvipsnames},hyperref={pdftex,pdfpagemode=UseNone,hidelinks,pdfdisplaydoctitle=true},usepdftitle=false]{beamer}
\usepackage{presentation}
% Enter title of presentation PDF:
\hypersetup{pdftitle={Minimalist LaTeX Template for Academic Presentations}}
% Enter link to PDF file with figures:
% \newcommand{\pdf}{figures.pdf}
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\x}{\mathbf{x}}
\DeclareMathOperator*{\y}{\mathbf{y}}

\newcommand{\MW}[1]{{\color{blue} Max: #1}}

\newcommand{\rmm}{\mathbf{m}}
\newcommand{\rmh}{\mathbf{h}}
\newcommand{\rmH}{\mathbf{H}}
\newcommand{\rmx}{\mathbf{x}}
\newcommand{\rmy}{\mathbf{y}}
\newcommand{\rmX}{\mathbf{X}}
\newcommand{\rmp}{\mathbf{p}}
\newcommand{\rmu}{\mathbf{u}}
\newcommand{\rmv}{\mathbf{v}}
\newcommand{\rmz}{\mathbf{z}}
\newcommand{\rmr}{\mathbf{r}}
\newcommand{\En}{\mathrm{E}(n)}

\begin{document}
\title{$\En$-Equivariant \newline Graph Neural Networks}
\information%
[https://arxiv.org/abs/2102.09844]%
{Victor Garcia Satorras, Emiel Hoogeboom, Max Welling}%
% {Location -- Date}
\frame{\titlepage}

% \subtitle{Efficient Learning in Multi-Dimensional Spaces}
% \author{Victor Garcia Satorras \and Emiel Hoogeboom \and Max Welling}
% \institute{UvA-Bosch Delta Lab, University of Amsterdam, Netherlands}
% \date{\today}

% \frame{\titlepage}

\begin{frame}
\frametitle{Introduction}
\begin{itemize}
    \item Deep learning should exploit symmetry in data.
    \item Examples of network with symmetries: Translation in CNNs, Permutation in GNNs.
    \item 3D data has Euclidean group symmetries (E(3)) that can be exploited.
    \item Current methods: Require higher-order representations; computationally expensive.
    \item This work: Presents a simpler, computationally efficient model with E(3). Also scalable to higher dimensions beyond 3D without a significant increase in computation.
\end{itemize}
\end{frame}

% 3. Background %
\begin{frame}
\frametitle{Background: E(3) Equivariance}
\begin{itemize}
      \item \textbf{Equivariance:} Function $\phi: X \to Y$ respects transformations in its input space $X$, mirroring them in output space $Y$: \begin{itemize}
      \item $\phi(T_g(\rmx)) = T_g(\phi(\rmx))$ where $T_g$ and $S_g$ are equivalent transformations associated with group $g$.\end{itemize}
      \item \textbf{Key equivariance properties:} Translation, Rotation (Reflection), and Permutation Equivariance.

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Background: Graph Neural Networks}
\begin{itemize}
      \item Graphs defined by nodes $(\mathcal{V})$ and edges $(\mathcal{E})$.
      \item Node embeddings evolve through layers using a message-passing scheme:
      \begin{equation}
        \begin{aligned}
        \mathbf{m}_{i j} & =\phi_e\left(\mathbf{h}_i^l, \mathbf{h}_j^l, a_{i j}\right) \\
        \mathbf{m}_i & =\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{i j} \\
        \mathbf{h}_i^{l+1} & =\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
        \end{aligned}
        \end{equation}
      \item $\phi_e$ and $\phi_h$ are the edge and node operations often being MLPs. Aggregation of messages and transformation are applied per node.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Equivariant Graph Neural Networks}
\begin{itemize}
      \item The paper's  model enhances traditional GNNs by incorporating equivariance.
      \item Updates node positions and features while maintaining structural and transformational integrity.
      \item \textbf{Key operation:} Equivariant updates of node positions using edge embeddings.
\end{itemize}
\begin{align}
\rmm_{ij} &=\phi_{e}\left(\rmh_{i}^{l}, \rmh_{j}^{l},\left\|\rmx_{i}^{l}-\rmx_{j}^{l}\right\|^{2}, a_{i j}\right) \\
\rmx_{i}^{l+1} &=\rmx_{i}^{l}+ C\sum_{j \neq i}\left(\rmx_{i}^{l}-\rmx_{j}^{l}\right) \phi_{x}\left(\rmm_{ij}\right)
\end{align}
\end{frame}

\begin{frame}
\frametitle{Analysis on $E(n)$ Equivariance}
\begin{itemize}
      \item Proof of equivariance under the $E(n)$ group: $$Q \mathbf{x}^{l+1}+g, \mathbf{h}^{l+1}=\operatorname{EGCL}\left(Q \mathbf{x}^l+g, \mathbf{h}^l\right)$$.
      \item Focus on translational and rotational symmetry for node coordinates: invariance translation, equivariance of rotation, etc. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Extending EGNNs for Vector Type Representations}
\begin{itemize}
      \item Incorporation of momentum for dynamic systems, enhancing the prediction of particle movement, extending such that $\mathbf{h}^{l+1}, \mathbf{x}^{l+1}, \mathbf{v}^{l+1}=\operatorname{EGCL}\left[\mathbf{h}^l, \mathbf{x}^l, \mathbf{v}^{\text {init }}, \mathcal{E}\right]$. 
      \item Coordinate update (eq. 3) in two steps: compute the velocity and then update the position:
      \begin{equation}
        \begin{aligned}
    \mathbf{v}_{i}^{l+1}&= \phi_{v}\left(\rmh_{i}^l\right)\rmv_{i}^\text{init} + C\sum_{j \neq i}\left(\mathbf{x}_{i}^{l}-\mathbf{x}_{j}^{l}\right) \phi_{x}\left(\mathbf{m}_{ij}\right) \\
    \rmx_{i}^{l+1} &=\mathbf{x}_{i}^{l}+ \mathbf{v}_i^{l+1}
        \end{aligned}
    \end{equation}
        
    \item Observations: $\phi_v$ maps $\mathbf{h}_i^l$ to a scalar value scaling velocity. Zero initial velocity $\left(\mathbf{v}_i^{\text {init }}=0\right)$, equation becomes same as eq. 3.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inferring Edges in Point Clouds}
\begin{itemize}
      \item Addressing edge detection in unstructured data.
      \item Use of a soft-attention mechanism to infer connections:
    \begin{equation}
        \begin{aligned}
    \mathbf{m}_i=\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{i j}=\sum_{j \neq i} e_{i j} \mathbf{m}_{i j} \\
    e_{ij} \approx \phi_{inf}(\rmm_{ij})
        \end{aligned}
    \end{equation}
      \item Facilitates dynamic and adaptive network topology.
\end{itemize}
\end{frame}

% 4. Related Work %
\begin{frame}
    \frametitle{Related Work Overview - Table 1}
    \begin{itemize}
        \item \textbf{General Equivariance:} (Cohen et al., 2016; Weiler et al., 2019).
        \item \textbf{E(3) and SE(3) Equivariance:}
        \begin{itemize}
        \item Utilizing spherical harmonics for higher-order transformations (Thomas et al., 2018; Fuchs et al., 2020).
        \item Computational expense and limitation to 3D spaces.
        \end{itemize}
        \item \textbf{Message Passing in Molecular Data:}
        \begin{itemize}
        \item Has permutation equivariance, but lacks translation or rotation (Gilmer et al., 2017).
        \item $\En$ invariant networks considering relative distances (Sch√ºtt et al., 2017).
        \end{itemize}
        \item \textbf{Innovative Methods:} Incorporating angles and directional information through modified message passing (Klicpera et al., 2020).
        % DimeNet
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Comparison with Existing Methods}
\begin{itemize}
    %\item Comparative analysis of message passing mechanisms detailed in \cite{gilmer2017neural}.
    \item \textbf{Distinguishing Features of EGNN:}
    \begin{itemize}
    \item Unlike other methods, EGNN updates both node embeddings and coordinates.
    \item Maintains $\En$ equivariance efficiently without costly operations like spherical harmonics.
    \end{itemize}
    \item \textbf{Some Comparisons:}
    \begin{itemize}
    \item Radial Field (Kohler et al., 2019) method focuses solely on positional data, limiting feature dynamics across nodes.
    \item Tensor Field Networks (Thomas et al., 2018) offer comprehensive feature propagation but at a high computational cost.
    \end{itemize}
\end{itemize}
% \includegraphics[width=0.8\textwidth]{Figures/comparison_table.pdf} % Assuming you have a comparative table figure
\end{frame}

\begin{frame}
\frametitle{Experiment 1: Modelling a Dynamical System - N-body System}
\begin{itemize}
    \item \textbf{Context:} Dynamical systems are fundamental in various domains such as control systems and physical simulations.
    \item \textbf{Experiment Setup:} Extended the Charged Particles N-body experiment to 3D. System involves 5 particles with charges, interacting through basic physical laws.
    \item \textbf{Data:} 3,000 trajectories for training, 2,000 for validation, and 2,000 for testing. Each trajectory spans 1,000 timesteps.
    \item \textbf{Task:} Predict positions of particles after 1,000 timesteps based on initial conditions.
\end{itemize}
\end{frame}    

\begin{frame}
\frametitle{N-body System: Implementation and Results}
\begin{itemize}
    \item \textbf{Implementation:} Utilized EGNN with velocity extension. Charges input as edge attributes. Compared with several baselines and other equivariant methods.
    \item \textbf{Performance:} Achieved the lowest MSE, outperforming others by 32\%.
\end{itemize}
% \includegraphics[width=\textwidth]{Figures/n_body.png}
% \caption{Performance comparison and MSE across different training data volumes.}
\end{frame}


\begin{frame}
\frametitle{Experiment 2: Representation Learning - Graph Autoencoder}
\begin{itemize}
    \item \textbf{Objective:} Learn unsupervised representations of graphs in a continuous latent space.
    \item \textbf{Approach:} Used EGNN to construct an Equivariant Graph Autoencoder.
    \item \textbf{Dataset and Task:} Embedding of graphs into latent spaces; focused on reconstructing adjacency matrix without node features.
\end{itemize}
\end{frame}
    
\begin{frame}
\frametitle{Graph Autoencoder: Implementation and Results}
\begin{itemize}
    \item \textbf{Implementation:} Autoencoder framework with EGNN encoder and a simple decoder for adjacency matrix reconstruction. Addressed symmetries in graphs without node features by introducing noise.
    \item \textbf{Results:} Demonstrated superior performance in embedding and reconstructing graphs, significantly reducing errors.
\end{itemize}
% \includegraphics[width=0.8\textwidth]{Figures/autoencoder_results.png}
% \caption{Comparison of Binary Cross Entropy and F1 scores across models.}
\end{frame}


\begin{frame}
\frametitle{Experiment 2: Molecular Data Prediction - QM9 Dataset}
\begin{itemize}
    \item \textbf{Dataset:} QM9, standard for chemical property prediction. Contains small molecules with varied atomic properties.
    \item \textbf{Task:} Predict 12 chemical properties, which are invariant to spatial transformations of molecule configurations.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{QM9: Implementation and Results}
\begin{itemize}
    \item \textbf{Implementation:} EGNN implemented without updating particle positions, focusing on edge inference and molecular properties prediction.
    \item Competitive results, achieving high accuracy across all tested properties, highlighting the model's efficacy and simplicity.
\end{itemize}
% \includegraphics[width=0.8\textwidth]{Figures/qm9_results.png}
% \caption{Performance metrics across various molecular properties.}
\end{frame}
                                    
\end{document}